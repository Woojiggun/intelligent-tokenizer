# 어텐션에 어휘는 필요 없다: 범용 토큰화를 위한 바이트 레벨 학습

**우진현**  
독립 연구자  
ggunio5782@gmail.com

*제목은 "Attention Is All You Need" (Vaswani et al., 2017)에 대한 오마주입니다*

## 초록

현재의 토큰화 방법들은 언어별 규칙과 사전 정의된 어휘에 크게 의존하여, 새로운 언어나 도메인에 대한 적용성이 제한적입니다. 우리는 언어 규칙이나 어휘 파일 없이 바이트 레벨에서 텍스트를 처리하는 순수 학습 기반 접근법인 인텔리전트 토크나이저를 제시합니다. 우리 모델은 계층적 어텐션 메커니즘을 통해 원시 UTF-8 바이트로부터 직접 언어 패턴을 학습하여, 204개 언어에서 어휘 없는 토큰화를 달성했습니다. 핵심 혁신은 토큰화를 언어 모델로부터 분리하여 효율적인 리소스 활용과 향상된 일반화를 가능하게 한 것입니다. 단 1억 500만 개의 파라미터로 영어에서 95%의 복원 정확도를 달성하면서, 256바이트 청킹을 통해 실시간 처리 능력을 유지합니다. 이 연구는 수동 설정 없이 모든 텍스트 형식에 적응할 수 있는 범용적이고 언어 독립적인 AI 시스템을 향한 한 걸음을 나타냅니다.

## 1. 서론

### 1.1 동기

현대 자연어 처리 시스템은 텍스트를 모델이 사용할 수 있는 이산 단위로 변환하는 토크나이저에 의존합니다. 그러나 기존 토큰화 접근법들은 근본적인 한계를 가지고 있습니다:

BPE(Byte Pair Encoding)는 학습 코퍼스에 특화된 사전 계산된 병합 규칙이 필요하여, 분포 외 텍스트에 대해 유연하지 못합니다. SentencePiece는 언어별 전처리 단계가 필요하며, 본 적 없는 문자나 혼합 언어 콘텐츠에서 실패합니다. MegaByte와 같은 최근 신경망 접근법은 유망하지만, 실용적 배포를 제한하는 막대한 계산 자원(15억 파라미터)이 필요합니다.

AI 시스템이 다양한 언어, 코드, 이모지, 새로운 텍스트 형식을 만나면서 전 세계적으로 확장됨에 따라, 이러한 한계는 중요해집니다. 범용적이고 적응 가능한 토큰화 방법의 필요성이 그 어느 때보다 절실합니다.

### 1.2 우리의 접근법

우리는 접근법을 차별화하는 세 가지 핵심 혁신을 제안합니다:

**첫째**, 실시간 스트리밍 애플리케이션을 가능하게 하는 256바이트 청크 처리를 구현합니다. 배치 처리 토크나이저와 달리, 우리 시스템은 텍스트를 점진적으로 처리할 수 있어 대화형 애플리케이션과 엣지 디바이스에 적합합니다.

**둘째**, 토큰화 로직을 언어 모델로부터 분리하여, 전통적으로 LLM이 처리하던 전처리 작업을 오프로드합니다. 이 분리는 계산 오버헤드를 줄이고 각 구성 요소의 전문화된 최적화를 가능하게 합니다.

**셋째**, 순수 바이트 레벨 계층적 학습 구조는 고유한 멀티모달 확장성을 제공합니다. 사전 정의된 토큰이 아닌 바이트 간의 관계를 학습함으로써, 모델은 모든 UTF-8 인코딩된 콘텐츠에 자연스럽게 적응합니다.

**명확히 하면**: "어휘가 없다"는 학습된 서브워드 어휘가 없다는 의미입니다; 우리 모델은 원시 UTF-8 바이트에서 직접 작동합니다.

### 1.3 언어 모델(LLM) 혁명적 크대화의 원리

기존의 LLM은 텍스트를 이해하고 생성하기 위해 내부적으로 토크나이징 로직까지 함께 처리해야 합니다. 이 과정에서 모델의 상당한 연산 자원이 토큰화 및 어휘집(vocabulary) 관리에 할당되죠. 그러나 지능형 토크나이저는 이 역할을 별도의 경량 모델이 맡아주기 때문에 LLM은 순수하게 추론과 언어 이해에만 집중할 수 있습니다.

마치 사람이 요리를 할 때 식재료 손질(토크나이징)과 실제 요리(추론)를 분리하는 것과 같습니다. 식재료가 모두 준비된 상태에서 요리에만 집중하면 더 빠르고 효율적으로 맛있는 음식을 만들 수 있죠. 마찬가지로, 지능형 토크나이저가 미리 바이트 수준의 패턴을 학습하여 의미 있는 단위(semantic unit)로 변환한 뒤 이를 LLM에 전달하면, LLM은 번거로운 전처리 과정 없이 바로 추론에 돌입할 수 있습니다.

동일 규모에서 더 강력한 추론

이러한 분리 전략 덕분에 다음과 같은 이점을 얻을 수 있습니다:

• **연산 효율성 증가**: LLM은 복잡한 토크나이징 규칙이나 어휘 파일 관리에 드는 연산을 절약할 수 있습니다. 이로 인해 동일한 하드웨어 자원을 가지고도 더 많은 추론 작업을 처리하거나, 더 깊고 복잡한 추론을 수행할 수 있게 됩니다.

• **리소스 최적화**: 지능형 토크나이저는 1억 5백만 개의 매개변수만으로 작동하는 경량 모델입니다. 반면, 기존의 토크나이징 기능을 내장한 LLM은 더 많은 매개변수를 필요로 합니다. 이 둘을 분리하면 전체 시스템의 리소스를 더 효율적으로 배분할 수 있습니다.

• **병렬 처리 가능성**: 토크나이징과 추론이 독립적인 프로세스가 되므로, 각각을 병렬로 처리하여 전체 시스템의 응답 속도를 향상시킬 수 있습니다. 특히 256바이트 청크 단위로 처리하는 방식은 스트림 애플리케이션에서 실시간성을 더욱 강화합니다.

결론적으로, 이 전략은 LLM의 '언어적' 지능을 순수하게 높이는 데 집중하게 함으로써 같은 모델 규모에서도 훨씬 더 강력하고 효율적인 성능을 기대할 수 있게 하는 핵심적인 차별점입니다.

### 1.4 기여

우리 연구는 세 가지 주요 기여를 합니다:

1. 언어 규칙이나 어휘 파일 없이 순수하게 데이터로부터 효과적인 토큰화를 학습할 수 있음을 보여주며, 204개 언어에서 언어 독립적 처리를 달성합니다.

2. 실시간 처리 능력을 유지하는 경량 모델(1억 500만 파라미터)을 개발하여, 리소스가 제한된 환경에서도 고급 토큰화를 가능하게 합니다.

3. Flores-200 데이터셋 학습을 통해, 인간이 정의한 규칙이 아닌 학습된 바이트 패턴으로부터 의미가 나타나는 언어 독립적 AI 시스템의 기초를 확립합니다.

## 2. 관련 연구

### 2.1 전통적 토크나이저

고전적 토큰화 방법들이 수십 년간 NLP를 지배해왔습니다. BPE (Sennrich et al., 2016)는 통계 분석을 기반으로 빈번한 바이트 쌍을 반복적으로 병합합니다. WordPiece (Schuster & Nakajima, 2012)는 서브워드 선택을 위해 우도 기반 접근법을 사용합니다. SentencePiece (Kudo & Richardson, 2018)는 유니그램 언어 모델링을 통해 언어 독립적 토큰화를 제공합니다. 효과적이지만, 이러한 방법들은 사전 계산된 어휘가 필요하고 어휘 외 토큰에 어려움을 겪습니다.

### 2.2 최근 신경망 접근법

최근 연구는 학습 기반 토큰화를 탐구합니다. MegaByte (Meta, 2023)는 패치 기반 어텐션을 가진 15억 파라미터 모델을 사용하여 바이트 레벨에서 시퀀스를 처리합니다. T-FREE (2024)는 해시 기반 단어 임베딩을 통해 토크나이저를 제거하지만 단어 경계 탐지가 필요합니다. BlockBPE (2025)는 GPU 병렬화를 통해 BPE를 가속화하지만 어휘 의존성을 유지합니다.

### 2.3 우리의 위치

우리 접근법은 기존 방법들과 근본적으로 다릅니다. MegaByte의 거대한 모델(15억 파라미터)과 달리, 우리는 아키텍처 효율성을 통해 1억 500만 파라미터로 비교 가능한 결과를 달성합니다. T-FREE의 해시 기반 접근법과 비교하여, 우리는 의미 관계를 포착하는 연속 표현을 학습합니다. 전통적 방법을 최적화하는 BlockBPE와 달리, 우리는 순수 학습을 통해 어휘 요구사항을 완전히 제거합니다.

## 3. 방법

### 3.1 아키텍처 개요

우리 시스템은 간단한 파이프라인을 통해 텍스트를 처리합니다:
```
입력 텍스트 → UTF-8 바이트 → 256바이트 청크 → 인코더 → 디코더 → 복원
```

각 구성 요소는 언어적 가정 없이 작동하며, 바이트 시퀀스로부터 직접 패턴을 학습합니다.

### 3.2 핵심 혁신: 계층적 관계 학습

**계층적 관계 기반 학습**: 각 임베딩 레이어에서 크로스 어텐션 메커니즘을 위치 임베딩과 결합하여 바이트 관계를 학습합니다. 이 접근법은 데이터의 다중 스케일 패턴을 포착하여 복원 정확도와 일반화를 모두 향상시킵니다.

**계층적 의미 단위 그룹화**: 모델은 학습 중 발견된 언어적 경계를 기반으로 바이트를 그룹화하는 것을 학습합니다. 한국어 텍스트의 경우, 명시적 형태소 분석 없이 단어 또는 구 수준 임베딩의 출현을 기대합니다.

### 3.3 ByteEncoder 아키텍처

우리 인코더는 5개 레이어에 걸쳐 점진적 차원을 사용합니다:
- 레이어 1-2: 기본 바이트 패턴 인식을 위한 384차원
- 레이어 3: 멀티바이트 시퀀스 학습을 위한 512차원
- 레이어 4: 형태소 수준 패턴 출현을 위한 640차원
- 레이어 5: 의미 표현을 위한 768차원

이 단계적 접근법은 모델 간결성을 유지하면서 효율적인 특징 추출을 가능하게 합니다.

### 3.4 학습 전략

우리는 세 가지 핵심 학습 전략을 사용합니다:
- 256바이트 청킹은 스트리밍 처리와 제한된 메모리 사용을 가능하게 합니다
- 스케줄된 샘플링을 가진 교사 강요는 학습 안정성과 생성 품질의 균형을 맞춥니다
- 다중 목적 손실 함수는 복원, 압축, 의미 보존을 동시에 최적화합니다

### 3.5 그룹화를 통한 확장된 컨텍스트

우리의 진행 중인 연구는 학습된 의미 그룹화를 통한 컨텍스트 확장을 탐구합니다. 토큰화와 추론 구성 요소를 분리함으로써, 효율성을 향상시키면서 지연 시간 제한을 극복합니다. 추론 LLM은 어휘 없이 임베딩에서 작동하여, 언어 독립성과 리소스 효율성을 보장합니다.

## 4. 실험

### 4.1 데이터셋

우리는 병렬 문장으로 204개 언어를 다루는 다국어 데이터셋인 Flores-200에서 학습합니다. 이 다양한 코퍼스는 쓰기 시스템 전반에 걸쳐 범용 바이트 패턴 학습을 가능하게 합니다.

### 4.2 학습 설정

제한적인 연구 환경으로 인해 단일 RTX 4070 GPU에서 학습을 수행했습니다. 이로 인해 학습 속도가 느렸으며, 실제 약 2일(24시간)의 학습 시간 동안 22 에포크까지만 진행할 수 있었습니다. 대규모 인프라에서 70-80% 추가 학습 시 상당한 성능 향상을 기대합니다:
- 배치 크기: 32
- 학습률: 코사인 어닐링을 가진 5e-5
- 그래디언트 누적: 4 스텝
- 실제 학습 시간: ~24시간 (하드웨어 제한)

### 4.3 평가 메트릭

우리는 네 가지 핵심 메트릭으로 모델을 평가합니다:
- **복원 정확도**: 정확한 바이트 시퀀스 복구율
- **압축 비율**: 의미 단위당 평균 바이트
- **추론 속도**: 초당 처리된 토큰
- **교차 언어 전이**: 보지 못한 언어 쌍에서의 성능

## 5. 결과

### 5.1 주요 결과

제한된 계산 자원과 진행 중인 학습으로 인해, 에포크 22의 예비 결과를 제시합니다:

| 언어 | 복원 정확도 |
|------|-----------|
| 영어 | 95%       |
| 한국어 | 70% (20에포크 시 97%) |
| 일본어 | 81%      |
| 중국어 | 7%       |
| 평균 (204개 언어) | 47% |

주의: 한국어는 단독 학습 시(에포크 1-20) 97% 정확도를 달성했으나, 다국어 학습 전환 후(에포크 21-22) 70%로 하락했습니다. 이 결과는 불균형한 데이터셋 분포를 가진 단일 RTX 4070에서의 학습을 반영합니다. 균형 잡힌 데이터와 확장된 학습으로 상당한 개선을 기대합니다.

### 5.2 고유한 장점

현재 한계에도 불구하고, 우리 접근법은 여러 장점을 보여줍니다:

**실시간 처리**: 256바이트 청킹은 완전한 문서가 아닌 도착하는 텍스트를 처리하여, 제한된 지연 시간으로 스트리밍 애플리케이션을 가능하게 합니다.

**경량 배포**: 1억 500만 파라미터로, 우리 모델은 휴대폰과 임베디드 시스템을 포함한 엣지 디바이스에서 실행됩니다.

**어휘 없는 작동**: 어휘 파일을 제거하면 메모리 공간을 줄이고 새로운 언어나 도메인에 즉시 적응할 수 있습니다.

### 5.3 상세 성능 분석

**교차 언어 성능 통찰**:
- **데이터 분포 영향**: 초기 학습(에포크 1-20)은 한국어 텍스트에 집중하여 97% 복원 정확도를 달성했습니다. 그러나 다국어 학습에서 심각한 데이터 불균형이 드러났습니다 - 초기 배치의 90%가 한국어였고, 다른 언어로 전환 시 치명적인 망각이 발생했습니다.
- **언어별 도전 과제**:
  - 한국어: 망각에도 불구하고 부분적 구조 이해(70%)를 유지하여 한글 바이트 패턴의 견고한 학습을 시사합니다
  - 중국어: 7% 정확도는 UTF-8 복잡성과 최소한의 학습 노출을 반영합니다
  - 일본어: 81% 성능은 혼합 문자 체계(히라가나/가타카나/한자)의 성공적 학습을 나타냅니다

**학습 역학**:
- **에포크 1-20**: 한국어 전용 학습으로 컴퓨팅 자원의 90%를 완벽한 복원 달성에 할애
- **에포크 21-22**: 다국어 전환으로 가중치 재분배가 발생하여 모델의 가소성을 보여주지만 균형 잡힌 커리큘럼 학습의 필요성을 강조합니다

### 5.4 미래 LLM에 대한 함의

예비 결과이지만, 우리 연구는 차세대 언어 모델에 혁명적 함의를 제시합니다:

**어휘 제거의 이점**:
- 현재 LLM은 초기 레이어의 70%를 토큰화 관련 처리에 할애합니다. 우리 접근법은 이 자원을 실제 추론으로 재배치합니다.
- 바이트 수준 이해는 진정한 제로샷 전이를 가능하게 합니다 - 모델은 언어별 토큰이 아닌 보편적 패턴을 학습합니다.

**효율성 향상**:
- **손실 없는 압축**: 일부 언어의 낮은 복원율에도 불구하고 의미 정보는 보존됩니다. 모델은 정확한 바이트 복구보다 의미를 우선시하도록 학습합니다.
- **동적 세분화**: 고정 어휘와 달리, 우리 모델은 문맥에 따라 토큰 경계를 조정합니다 - 이름에는 문자 수준, 일반 표현에는 구 수준으로.

### 5.5 한계

우리는 현재 구현의 여러 한계를 인정합니다:
- UTF-8 인코딩 복잡성으로 인해 중국어 문자 처리가 여전히 어렵습니다
- 데이터셋 불균형이 언어별 성능에 영향을 미칩니다
- 제한된 계산 자원이 전체 모델 잠재력을 방해합니다

## 6. 논의

### 6.1 중국어 성능 분석

낮은 중국어 정확도(7%)는 여러 요인에서 비롯됩니다:
- **UTF-8 인코딩 복잡성**: 중국어 문자는 3-4바이트가 필요하여, 현재 256바이트 윈도우가 효과적으로 모델링하기 어려운 긴 의존성 체인을 만듭니다
- **학습 데이터 불균형**: 중국어는 학습 배치의 5% 미만을 차지하여 복잡한 표의문자 패턴 학습에 불충분했습니다
- **아키텍처 한계**: 현재 모델은 가변 길이 멀티바이트 시퀀스 처리를 위한 특화 메커니즘이 부족합니다

미래 해결책:
- 균형 잡힌 언어 샘플링을 통한 커리큘럼 학습
- 감지된 스크립트 유형에 따른 적응형 윈도우 크기
- CJK 언어를 위한 계층적 바이트 그루핑

### 6.2 미래 응용

우리의 어휘 없는 접근법은 여러 유망한 응용을 가능하게 합니다:

**LLM 통신 계층**: 토큰화 오버헤드 없이 모델 간 직접 임베딩 기반 통신.

**RAG 시스템**: 사전 정의된 토큰이 아닌 학습된 바이트 패턴을 사용한 의미 검색.

**엣지 AI**: 프라이버시 보호 애플리케이션을 위한 온디바이스 추론.

**복소수 값 임베딩**: 향후 연구는 더 풍부한 의미 인코딩을 위한 복소수 표현을 탐구할 것입니다.

**보편적 통신 프로토콜**: 우리의 어휘 없는 접근법은 토큰화 오버헤드 없이 원활한 모델 상호 운용성을 가능하게 하는 보편적 AI 통신 표준을 확립할 수 있습니다.

## 7. 결론

우리는 언어 규칙이나 어휘 파일 없이 UTF-8 바이트에서 직접 작동하는 순수 학습 기반 토큰화 접근법인 인텔리전트 토크나이저를 제시했습니다. 학습 한계에도 불구하고, 우리의 1억 500만 파라미터 모델은 실시간 처리 능력을 유지하면서 영어에서 95%의 복원 정확도를 달성합니다. 이 연구는 범용 AI 시스템을 위한 어휘 없는, 언어 독립적 토큰화의 실현 가능성을 보여줍니다. 우리는 커뮤니티 관심에 따라 구현을 오픈소스화할 계획이며, 진정으로 범용적인 텍스트 처리 시스템의 협력적 개발을 가능하게 할 것입니다.

## 감사의 말

우리에게 제목 영감을 준 Vaswani et al. (2017)에 감사드립니다. 이 연구는 중요한 다국어 학습 데이터를 제공한 Meta AI의 Flores-200 데이터셋 없이는 불가능했을 것입니다. 신경망 토큰화에서 선구적인 작업을 한 오픈소스 커뮤니티와 개발 단계에서 구현 도움을 준 Claude (Anthropic)에게 특별히 감사드립니다. AI 도움을 받았지만, 모든 핵심 아이디어, 실험 설계, 전략적 결정은 저자의 단독 기여입니다. 또한 탁월한 딥러닝 프레임워크를 제공한 PyTorch 팀에게도 감사드립니다.

## 참고문헌

1. Sennrich, R., Haddow, B., & Birch, A. (2016). Neural machine translation of rare words with subword units. ACL.

2. Schuster, M., & Nakajima, K. (2012). Japanese and Korean voice search. ICASSP.

3. Kudo, T., & Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. EMNLP.

4. Meta AI (2023). MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. arXiv:2305.07185.

5. T-FREE Authors (2024). Tokenizer-Free Generative LLMs via Sparse Representations. arXiv:2406.19223.

6. BlockBPE Authors (2025). Parallel BPE Tokenization. arXiv:2507.11941.

7. NLLB Team (2022). No Language Left Behind: Scaling Human-Centered Machine Translation. Meta AI.

8. Vaswani, A., et al. (2017). Attention is all you need. NeurIPS.

9. Devlin, J., et al. (2019). BERT: Pre-training of deep bidirectional transformers. NAACL.

10. Brown, T., et al. (2020). Language models are few-shot learners. NeurIPS.

11. Vaswani, A., et al. (2017). Attention is all you need. NeurIPS. (*제목 영감*)