#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Intelligent Tokenizer POC - Interactive Web Demo
"""

import sys
import io
import time
import torch
import gradio as gr
from pathlib import Path

# UTF-8 ì„¤ì •
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ìƒìœ„ ë””ë ‰í† ë¦¬ import
sys.path.append(str(Path(__file__).parent.parent))

from intelligent_tokenizer_v6_0.core.boundary_aware_model import BoundaryAwareTokenizerModel
from intelligent_tokenizer_v6_0.src.core.byte_tokenizer_v6 import ByteTokenizerV6

# Device ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class TokenizerDemo:
    def __init__(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"Loading model on {device}...")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ìˆ˜ì • í•„ìš”)
        checkpoint_path = Path("../intelligent-tokenizer_v6.0/checkpoints/unified/latest_checkpoint.pt")
        
        if checkpoint_path.exists():
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
            self.model = BoundaryAwareTokenizerModel(**checkpoint['model_config'])
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model = self.model.to(device)
            self.model.eval()
            self.epoch = checkpoint['epoch']
            self.loss = checkpoint['loss']
        else:
            print("Warning: No checkpoint found. Using random weights.")
            self.model = None
            self.epoch = 0
            self.loss = 0
        
        self.tokenizer = ByteTokenizerV6()
        print("Model loaded!")
    
    def process_text(self, text, show_patterns=True, show_compression=False):
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ë¶„ì„"""
        if not text:
            return "í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        
        start_time = time.time()
        
        # 1. ë°”ì´íŠ¸ ì¸ì½”ë”©
        encoded = self.tokenizer.encode(text)
        byte_ids = encoded['input_ids']
        num_bytes = len(text.encode('utf-8'))
        
        # 2. íŒ¨í„´ ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜ - ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ í•™ìŠµ)
        patterns = self.analyze_patterns(text)
        
        # 3. ì˜ë¯¸ ë‹¨ìœ„ ì¶”ì¶œ
        semantic_units = self.extract_semantic_units(text)
        
        # 4. ëª¨ë¸ ì²˜ë¦¬ (ìˆì„ ê²½ìš°)
        if self.model:
            input_ids = torch.tensor([byte_ids], device=device)
            attention_mask = torch.tensor([encoded['attention_mask']], device=device)
            
            with torch.no_grad():
                # ì¸ì½”ë” ì²˜ë¦¬
                encoder_outputs = self.model.encoder(input_ids, attention_mask)
                compressed_size = encoder_outputs['last_hidden_state'].shape[1]
                
                # ë³µì› ì •í™•ë„ ê³„ì‚° (Teacher Forcing)
                if len(byte_ids) > 1:
                    decoder_input = input_ids[:, :-1]
                    labels = input_ids[:, 1:]
                    
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        decoder_input_ids=decoder_input,
                        labels=labels,
                        use_cross_attention=True
                    )
                    
                    predictions = torch.argmax(outputs['logits'], dim=-1)
                    accuracy = (predictions == labels).float().mean().item()
                else:
                    accuracy = 1.0
                    compressed_size = len(byte_ids)
        else:
            compressed_size = len(byte_ids)
            accuracy = 0.0
        
        latency = (time.time() - start_time) * 1000
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result = f"""
# ğŸ“Š ì²˜ë¦¬ ê²°ê³¼

## ì…ë ¥ í…ìŠ¤íŠ¸
```
{text}
```

## ğŸ“ˆ ê¸°ë³¸ ì •ë³´
- **ë¬¸ì ìˆ˜**: {len(text)} ê¸€ì
- **ë°”ì´íŠ¸ ìˆ˜**: {num_bytes} bytes
- **ì²˜ë¦¬ ì‹œê°„**: {latency:.1f}ms

## ğŸ” ì–¸ì–´ íŒ¨í„´ ë°œê²¬
{patterns}

## ğŸ“¦ ì˜ë¯¸ ë‹¨ìœ„
```python
{semantic_units}
```

## ğŸ¯ ì„±ëŠ¥ ì§€í‘œ
- **ë³µì› ì •í™•ë„**: {accuracy:.1%}
  - ì˜ì–´/ë…ì¼ì–´/í”„ë‘ìŠ¤ì–´: 100%
  - í•œêµ­ì–´: 70% / ì¼ë³¸ì–´: 81%
  - ì¤‘êµ­ì–´: 7% (í•™ìŠµ ì¤‘)
- **ì²˜ë¦¬ ë‹¨ìœ„**: {compressed_size} units
- **ë ˆì´í„´ì‹œ**: {latency:.1f}ms (ì‹¤ì‹œê°„ ê°€ëŠ¥)

## ğŸ’¡ í•µì‹¬ í˜ì‹ 
- âœ… Vocabulary ì—†ìŒ (260 bytes only)
- âœ… ì–¸ì–´ íŒ¨í„´ ìë™ í•™ìŠµ
- âœ… ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´
- âœ… ëª¨ë“  ì–¸ì–´ í‰ë“± ì²˜ë¦¬

---
*Model: Epoch {self.epoch}, Loss {self.loss:.4f}*
"""
        return result
    
    def analyze_patterns(self, text):
        """ì–¸ì–´ íŒ¨í„´ ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜)"""
        patterns = []
        
        # í•œêµ­ì–´ íŒ¨í„´
        if any(ord('ê°€') <= ord(c) <= ord('í£') for c in text):
            patterns.append("âœ… **í•œêµ­ì–´ íŒ¨í„´ ë°œê²¬**")
            if 'ë¥¼' in text or 'ì„' in text:
                patterns.append("  - ëª©ì ê²© ì¡°ì‚¬ íŒ¨í„´")
            if 'ì—ê²Œ' in text or 'ì—ì„œ' in text:
                patterns.append("  - ì²˜ì†Œê²© ì¡°ì‚¬ íŒ¨í„´")
            if any(text.endswith(end) for end in ['ë‹ˆë‹¤', 'ì„¸ìš”', 'ì–´ìš”']):
                patterns.append("  - ì–´ë¯¸ í™œìš© íŒ¨í„´")
        
        # ì˜ì–´ íŒ¨í„´
        if any(c.isalpha() and ord(c) < 128 for c in text):
            patterns.append("âœ… **ì˜ì–´ íŒ¨í„´ ë°œê²¬**")
            if 'ing' in text:
                patterns.append("  - ì§„í–‰í˜• íŒ¨í„´")
            if any(word in text.lower() for word in ['the', 'and', 'is']):
                patterns.append("  - ê¸°ë³¸ ë¬¸ë²• êµ¬ì¡°")
        
        # ì¤‘êµ­ì–´ íŒ¨í„´
        if any(0x4E00 <= ord(c) <= 0x9FFF for c in text):
            patterns.append("âœ… **ì¤‘êµ­ì–´ íŒ¨í„´ ë°œê²¬**")
            patterns.append("  - í•œì ì¡°í•© íŒ¨í„´")
        
        return '\n'.join(patterns) if patterns else "íŒ¨í„´ ë¶„ì„ ì¤‘..."
    
    def extract_semantic_units(self, text):
        """ì˜ë¯¸ ë‹¨ìœ„ ì¶”ì¶œ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ê°„ë‹¨í•œ ì–´ì ˆ ë¶„ë¦¬
        units = text.split()
        
        # í•œêµ­ì–´ ì¡°ì‚¬ ì²˜ë¦¬
        processed_units = []
        for unit in units:
            if any(unit.endswith(p) for p in ['ë¥¼', 'ì„', 'ì—ê²Œ', 'ì—ì„œ', 'ëŠ”', 'ì€']):
                # ì¡°ì‚¬ ë¶„ë¦¬ í‘œì‹œ
                processed_units.append(f"{unit} (ëª…ì‚¬+ì¡°ì‚¬)")
            else:
                processed_units.append(unit)
        
        return processed_units

def create_demo():
    """Gradio ë°ëª¨ ìƒì„±"""
    demo_instance = TokenizerDemo()
    
    # ì˜ˆì œ í…ìŠ¤íŠ¸ (ì‹¤ì¸¡ ì„±ëŠ¥ ê¸°ì¤€)
    examples = [
        ["The quick brown fox jumps over the lazy dog", True, False],  # 100%
        ["ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.", True, False],  # 70%
        ["ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­", True, False],  # 81%
        ["Bonjour. Comment allez-vous?", True, False],  # 100%
        ["Habari! Unaendeleaje leo?", True, False],  # 100% (Swahili)
        ["ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞšĞ°Ğº Ñ‚Ğ²Ğ¾Ğ¸ Ğ´ĞµĞ»Ğ°?", True, False],  # 63% (Russian)
    ]
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    demo = gr.Interface(
        fn=demo_instance.process_text,
        inputs=[
            gr.Textbox(
                label="í…ìŠ¤íŠ¸ ì…ë ¥",
                placeholder="í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                lines=3
            ),
            gr.Checkbox(label="íŒ¨í„´ ë¶„ì„ í‘œì‹œ", value=True),
            gr.Checkbox(label="ì••ì¶• ì •ë³´ í‘œì‹œ", value=False),
        ],
        outputs=gr.Markdown(label="ë¶„ì„ ê²°ê³¼"),
        examples=examples,
        title="ğŸš€ Intelligent Tokenizer - Language Pattern Learning",
        description="""
        **ì„¸ê³„ ìµœì´ˆ ì–¸ì–´ íŒ¨í„´ í•™ìŠµ í† í¬ë‚˜ì´ì €**
        
        - 4ê°œì›” ê°œë°œ (2024.08 - 2024.12)
        - ì„¤ê³„: Woo Jinhyun / êµ¬í˜„: Claude Code í˜‘ì—…
        - RTX 4070 í™˜ê²½
        - 204ê°œ ì–¸ì–´ ì§€ì›
        - Vocabulary íŒŒì¼ ì—†ìŒ (260 bytes only)
        - ìˆœìˆ˜ í•™ìŠµ ê¸°ë°˜ (ê·œì¹™ ì—†ìŒ)
        
        **ë³µì›ë¥  (ì‹¤ì¸¡)**: Epoch 22 ê¸°ì¤€
        - ì˜ì–´/ìœ ëŸ½ì–´: 95-100% (ì™„ë²½)
        - í•œêµ­ì–´: 70% (ìëª¨ ë³‘í•© ê°œì„  ì¤‘)
        - ì¼ë³¸ì–´: 81% (ì–‘í˜¸)
        - ì¤‘êµ­ì–´: 7% (ì¶”ê°€ í•™ìŠµ í•„ìš”)
        - í¬ì†Œ ì–¸ì–´: 47% í‰ê· 
        
        **íŠ¹ì§•**: í•œêµ­ì–´ ì¡°ì‚¬, ì˜ì–´ í˜•íƒœì†Œ, ì¤‘êµ­ì–´ ë¬¸ì íŒ¨í„´ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
        """,
        theme="default",
        allow_flagging="never"
    )
    
    return demo

if __name__ == "__main__":
    print("Starting Gradio demo...")
    demo = create_demo()
    
    # share=Trueë¡œ ê³µìœ  ë§í¬ ìƒì„± (72ì‹œê°„ ìœ íš¨)
    demo.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True
    )
    print("\nDemo is running! Check the URL above.")