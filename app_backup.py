#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Intelligent Tokenizer - Hugging Face Space Demo
"""

import gradio as gr
import torch
import torch.nn as nn
import time
from huggingface_hub import hf_hub_download

# ByteTokenizer í´ë˜ìŠ¤ ì •ì˜
class ByteTokenizerV6:
    def __init__(self, max_seq_len=256):
        self.vocab_size = 260
        self.max_seq_len = max_seq_len
        self.PAD = 256
        self.BOS = 257
        self.EOS = 258  
        self.MASK = 259
    
    def encode(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë°”ì´íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
        byte_seq = list(text.encode('utf-8'))
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  (256 ë°”ì´íŠ¸ ì œí•œ)
        if len(byte_seq) > self.max_seq_len - 2:
            chunks = []
            for i in range(0, len(byte_seq), self.max_seq_len - 2):
                chunk = byte_seq[i:i + self.max_seq_len - 2]
                chunks.append([self.BOS] + chunk + [self.EOS])
            return chunks
        else:
            return [[self.BOS] + byte_seq + [self.EOS]]
    
    def decode(self, ids):
        """ë°”ì´íŠ¸ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        filtered = [id for id in ids if id < 256]
        try:
            return bytes(filtered).decode('utf-8', errors='replace')
        except:
            return "[ë””ì½”ë”© ì˜¤ë¥˜]"

# ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
class BoundaryAwareTokenizerModel(nn.Module):
    def __init__(self, vocab_size=260, hidden_size=768, num_encoder_layers=5, 
                 num_decoder_layers=6, num_heads=8, dropout=0.1, max_position_embeddings=256):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # Embedding layers
        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=256)
        self.position_embedding = nn.Embedding(max_position_embeddings, hidden_size)
        
        # Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=dropout,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)
        
        # Decoder
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=dropout,
            batch_first=True
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)
        
        # Output projection
        self.output_projection = nn.Linear(hidden_size, vocab_size)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src, tgt=None):
        # Encoder
        src_embeddings = self.embedding(src)
        seq_len = src.shape[1]
        positions = torch.arange(seq_len, device=src.device).unsqueeze(0).expand_as(src)
        src_embeddings = src_embeddings + self.position_embedding(positions)
        src_embeddings = self.dropout(src_embeddings)
        
        encoder_output = self.encoder(src_embeddings)
        
        if tgt is not None:
            # Decoder
            tgt_embeddings = self.embedding(tgt)
            tgt_len = tgt.shape[1]
            tgt_positions = torch.arange(tgt_len, device=tgt.device).unsqueeze(0).expand_as(tgt)
            tgt_embeddings = tgt_embeddings + self.position_embedding(tgt_positions)
            tgt_embeddings = self.dropout(tgt_embeddings)
            
            # Create causal mask
            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(tgt.device)
            
            decoder_output = self.decoder(tgt_embeddings, encoder_output, tgt_mask=tgt_mask)
            logits = self.output_projection(decoder_output)
            
            return logits
        else:
            return encoder_output

# ëª¨ë¸ ë¡œë“œ
device = torch.device('cpu')  # Hugging Face SpaceëŠ” ê¸°ë³¸ CPU
model = None
tokenizer = ByteTokenizerV6()

def load_model():
    global model
    try:
        # Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        model_file = hf_hub_download(
            repo_id="ggunio/intelligent-tokenizer-v6",
            filename="pytorch_model.bin"
        )
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        model = BoundaryAwareTokenizerModel(
            vocab_size=260,
            hidden_size=768,
            num_encoder_layers=5,
            num_decoder_layers=6,
            num_heads=8,
            dropout=0.1,
            max_position_embeddings=256
        ).to(device)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        state_dict = torch.load(model_file, map_location=device)
        model.load_state_dict(state_dict)
        model.eval()
        return "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"
    except Exception as e:
        return f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"

# ì´ˆê¸° ëª¨ë¸ ë¡œë“œ
load_status = load_model()
print(load_status)

def process_text(text):
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ë¶„ì„"""
    if not text:
        return "í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    
    if model is None:
        return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    start_time = time.time()
    
    # 1. ë°”ì´íŠ¸ ì¸ì½”ë”© (ì²­í¬ ì²˜ë¦¬)
    chunks = tokenizer.encode(text)
    num_bytes = len(text.encode('utf-8'))
    num_chunks = len(chunks)
    
    # 2. ëª¨ë¸ ì²˜ë¦¬ (ì²« ë²ˆì§¸ ì²­í¬ë§Œ)
    chunk = chunks[0]
    input_ids = torch.tensor([chunk], device=device)
    
    with torch.no_grad():
        # ì¸ì½”ë” ì²˜ë¦¬
        encoded = model(input_ids)
        
        # ë””ì½”ë”ë¡œ ë³µì›
        if len(chunk) > 1:
            decoder_input = input_ids[:, :-1]
            labels = input_ids[:, 1:]
            
            logits = model(input_ids, decoder_input)
            predictions = torch.argmax(logits, dim=-1)
            
            # ë³µì› í…ìŠ¤íŠ¸
            recovered_bytes = predictions[0].tolist()
            recovered_text = tokenizer.decode(recovered_bytes)
            
            # ì •í™•ë„ ê³„ì‚°
            accuracy = (predictions == labels).float().mean().item()
        else:
            recovered_text = text
            accuracy = 1.0
    
    latency = (time.time() - start_time) * 1000
    
    # ì–¸ì–´ ê°ì§€ ë° ì˜ˆìƒ ë³µì›ë¥ 
    lang_info = detect_language(text)
    
    # ê²°ê³¼ í¬ë§·íŒ…
    result = f"""
# ğŸ“Š ì²˜ë¦¬ ê²°ê³¼

## ì…ë ¥ í…ìŠ¤íŠ¸
```
{text[:256] + '...' if len(text) > 256 else text}
```

## ğŸ“ˆ ê¸°ë³¸ ì •ë³´
- **ë¬¸ì ìˆ˜**: {len(text)} ê¸€ì
- **ë°”ì´íŠ¸ ìˆ˜**: {num_bytes} bytes
- **ì²­í¬ ìˆ˜**: {num_chunks} {'(256 ë°”ì´íŠ¸ ì œí•œìœ¼ë¡œ ë¶„í• )' if num_chunks > 1 else ''}
- **ì²˜ë¦¬ ì‹œê°„**: {latency:.1f}ms

## ğŸ” ì–¸ì–´ ê°ì§€
{lang_info}

## ğŸ”„ ë³µì› ê²°ê³¼
```
{recovered_text[:256] + '...' if len(recovered_text) > 256 else recovered_text}
```
- **ë³µì› ì •í™•ë„**: {accuracy:.1%}

## ğŸ¯ ì„±ëŠ¥ ì§€í‘œ (Epoch 22 ê¸°ì¤€)
- ì˜ì–´/ë…ì¼ì–´/í”„ë‘ìŠ¤ì–´: 95-100%
- í•œêµ­ì–´: 70% (ìëª¨ ë³‘í•© ë³µì¡ì„±)
- ì¼ë³¸ì–´: 81%
- ì¤‘êµ­ì–´: 7% (ì¶”ê°€ í•™ìŠµ í•„ìš”)
- í¬ì†Œ ì–¸ì–´: í‰ê·  47%

## ğŸ’¡ í•µì‹¬ í˜ì‹ 
- âœ… Vocabulary ì—†ìŒ (260 bytes only)
- âœ… ì–¸ì–´ íŒ¨í„´ ìë™ í•™ìŠµ
- âœ… ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´
- âœ… ëª¨ë“  ì–¸ì–´ í‰ë“± ì²˜ë¦¬

---
*Note: 256 ë°”ì´íŠ¸ ì´ìƒì˜ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ë¡œ ë¶„í•  ì²˜ë¦¬ë©ë‹ˆë‹¤.*
"""
    return result

def detect_language(text):
    """ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€"""
    patterns = []
    
    # í•œêµ­ì–´
    if any(ord('ê°€') <= ord(c) <= ord('í£') for c in text):
        patterns.append("ğŸ‡°ğŸ‡· **í•œêµ­ì–´** - ì˜ˆìƒ ë³µì›ë¥ : 70%")
    
    # ì˜ì–´
    if any(c.isalpha() and ord(c) < 128 for c in text):
        patterns.append("ğŸ‡¬ğŸ‡§ **ì˜ì–´** - ì˜ˆìƒ ë³µì›ë¥ : 95-100%")
    
    # ì¤‘êµ­ì–´
    if any(0x4E00 <= ord(c) <= 0x9FFF for c in text):
        patterns.append("ğŸ‡¨ğŸ‡³ **ì¤‘êµ­ì–´** - ì˜ˆìƒ ë³µì›ë¥ : 7%")
    
    # ì¼ë³¸ì–´
    if any(0x3040 <= ord(c) <= 0x309F or 0x30A0 <= ord(c) <= 0x30FF for c in text):
        patterns.append("ğŸ‡¯ğŸ‡µ **ì¼ë³¸ì–´** - ì˜ˆìƒ ë³µì›ë¥ : 81%")
    
    # ì•„ëì–´
    if any(0x0600 <= ord(c) <= 0x06FF for c in text):
        patterns.append("ğŸ‡¸ğŸ‡¦ **ì•„ëì–´** - ì˜ˆìƒ ë³µì›ë¥ : 38%")
    
    # ëŸ¬ì‹œì•„ì–´
    if any(0x0400 <= ord(c) <= 0x04FF for c in text):
        patterns.append("ğŸ‡·ğŸ‡º **ëŸ¬ì‹œì•„ì–´** - ì˜ˆìƒ ë³µì›ë¥ : 63%")
    
    return '\n'.join(patterns) if patterns else "ì–¸ì–´ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ì˜ˆì œ í…ìŠ¤íŠ¸
examples = [
    "The quick brown fox jumps over the lazy dog",  # ì˜ì–´ 100%
    "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",  # í•œêµ­ì–´ 70%
    "ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­",  # ì¼ë³¸ì–´ 81%
    "Bonjour. Comment allez-vous?",  # í”„ë‘ìŠ¤ì–´ 100%
    "Habari! Unaendeleaje leo?",  # ìŠ¤ì™€íë¦¬ì–´ 100%
    "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞšĞ°Ğº Ñ‚Ğ²Ğ¾Ğ¸ Ğ´ĞµĞ»Ğ°?",  # ëŸ¬ì‹œì•„ì–´ 63%
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½",  # ì¤‘êµ­ì–´ 7%
]

# Gradio ì¸í„°í˜ì´ìŠ¤
demo = gr.Interface(
    fn=process_text,
    inputs=gr.Textbox(
        label="í…ìŠ¤íŠ¸ ì…ë ¥",
        placeholder="í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
        lines=3
    ),
    outputs=gr.Markdown(label="ë¶„ì„ ê²°ê³¼"),
    examples=examples,
    title="ğŸš€ Intelligent Tokenizer v6.0 - Language Pattern Learning",
    description="""
    **ì–¸ì–´ íŒ¨í„´ í•™ìŠµ í† í¬ë‚˜ì´ì €**
    
    - 4ê°œì›” ê°œë°œ (2024.08 - 2024.12)
    - ì„¤ê³„: Woo Jinhyun / êµ¬í˜„: Claude Code í˜‘ì—…
    - RTX 4070 í™˜ê²½ì—ì„œ 22 Epochs í•™ìŠµ
    - 204ê°œ ì–¸ì–´ ì§€ì› (Flores-200 ë°ì´í„°ì…‹)
    - Vocabulary íŒŒì¼ ì—†ìŒ (260 bytes only)
    - ìˆœìˆ˜ í•™ìŠµ ê¸°ë°˜ (ì–¸ì–´ë³„ ê·œì¹™ ì—†ìŒ)
    
    **ì‹¤ì¸¡ ë³µì›ë¥ ** (Epoch 22 ê¸°ì¤€):
    - ì˜ì–´/ìœ ëŸ½ì–´: 95-100% âœ…
    - í•œêµ­ì–´: 70% ğŸ”„ (ìëª¨ ë³‘í•© ê°œì„  ì¤‘)
    - ì¼ë³¸ì–´: 81% âœ…
    - ì¤‘êµ­ì–´: 7% âš ï¸ (ì¶”ê°€ í•™ìŠµ í•„ìš”)
    - í¬ì†Œ ì–¸ì–´: í‰ê·  47%
    
    **íŠ¹ì§•**: í•œêµ­ì–´ ì¡°ì‚¬, ì˜ì–´ í˜•íƒœì†Œ, ì¤‘êµ­ì–´ ë¬¸ì íŒ¨í„´ì„ ìˆœìˆ˜ í•™ìŠµìœ¼ë¡œ ë°œê²¬í•©ë‹ˆë‹¤.
    
    **ì£¼ì˜**: ì´ í”„ë¡œì íŠ¸ëŠ” ì‹¤í—˜ì  POCì´ë©°, ìƒìš© ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    
    GitHub: [Coming Soon] | Paper: [In Progress]
    """,
    theme="default",
    allow_flagging="never",
    cache_examples=False
)

if __name__ == "__main__":
    demo.launch()