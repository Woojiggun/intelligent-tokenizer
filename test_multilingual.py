#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë‹¤êµ­ì–´ ë³µì›ë¥  í…ŒìŠ¤íŠ¸ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ë¡œ ì¸¡ì •
"""

import torch
import sys
import io
from pathlib import Path
import json

# UTF-8 ì¸ì½”ë”© ì„¤ì •
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ìƒìœ„ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent.parent / "intelligent-tokenizer_v6.0"))

from core.boundary_aware_model import BoundaryAwareTokenizerModel
from src.core.byte_tokenizer_v6 import ByteTokenizerV6

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}\n")

# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
checkpoint_path = Path("../intelligent-tokenizer_v6.0/checkpoints/unified/latest_checkpoint.pt")
checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
model = BoundaryAwareTokenizerModel(**checkpoint['model_config'])
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(device)
model.eval()

tokenizer = ByteTokenizerV6()

print(f"Model loaded: Epoch {checkpoint['epoch']}, Loss {checkpoint['loss']:.4f}\n")
print("="*70)

def test_language(text, lang_name):
    """ì–¸ì–´ë³„ í…ŒìŠ¤íŠ¸"""
    encoded = tokenizer.encode(text)
    byte_ids = encoded['input_ids']
    
    # 256 ë°”ì´íŠ¸ ì œí•œ ì²´í¬
    if len(byte_ids) > 256:
        text = text[:80]  # ì¤„ì´ê¸°
        encoded = tokenizer.encode(text)
        byte_ids = encoded['input_ids']
    
    input_ids = torch.tensor([byte_ids], device=device)
    attention_mask = torch.tensor([encoded['attention_mask']], device=device)
    
    with torch.no_grad():
        # Teacher Forcing ì •í™•ë„
        if len(byte_ids) > 1:
            decoder_input = input_ids[:, :-1]
            labels = input_ids[:, 1:]
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                decoder_input_ids=decoder_input,
                labels=labels,
                use_cross_attention=True
            )
            
            predictions = torch.argmax(outputs['logits'], dim=-1)
            accuracy = (predictions == labels).float().mean().item()
            loss = outputs['loss'].item()
        else:
            accuracy = 1.0
            loss = 0.0
        
        # Autoregressive ìƒì„±
        generated = [tokenizer.BOS]
        max_len = min(len(byte_ids) * 2, 256)
        
        encoder_outputs = model.encoder(input_ids, attention_mask)
        encoder_hidden = encoder_outputs['last_hidden_state']
        
        for _ in range(max_len):
            decoder_input = torch.tensor([generated], dtype=torch.long, device=device)
            decoder_output = model.decoder(
                encoder_hidden,
                decoder_input,
                attention_mask
            )
            logits = decoder_output['logits'] if isinstance(decoder_output, dict) else decoder_output
            next_token = torch.argmax(logits[0, -1]).item()
            generated.append(next_token)
            if next_token == tokenizer.EOS:
                break
        
        # ë³µì› í…ìŠ¤íŠ¸
        valid_bytes = [b for b in generated[1:] if b < 256 and b != tokenizer.EOS]
        try:
            reconstructed = bytes(valid_bytes).decode('utf-8', errors='ignore')
            # ë¬¸ì ë‹¨ìœ„ ì •í™•ë„
            correct_chars = sum(1 for a, b in zip(text, reconstructed) if a == b)
            char_accuracy = correct_chars / max(len(text), 1)
        except:
            reconstructed = "[decode error]"
            char_accuracy = 0.0
    
    return {
        'text': text,
        'lang': lang_name,
        'tf_accuracy': accuracy,
        'char_accuracy': char_accuracy,
        'loss': loss,
        'reconstructed': reconstructed[:50] if len(reconstructed) > 50 else reconstructed
    }

# ë‹¤ì–‘í•œ ì–¸ì–´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
test_samples = {
    "ğŸŒ ì£¼ìš” ì–¸ì–´": {
        "English": "The quick brown fox jumps over the lazy dog",
        "í•œêµ­ì–´": "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”",
        "ä¸­æ–‡ç®€ä½“": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­æ•£æ­¥å§",
        "ä¸­æ–‡ç¹é«”": "ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œæˆ‘å€‘å»å…¬åœ’æ•£æ­¥å§",
        "æ—¥æœ¬èª": "ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­",
        "EspaÃ±ol": "Buenos dÃ­as. Â¿CÃ³mo estÃ¡ usted?",
        "FranÃ§ais": "Bonjour. Comment allez-vous aujourd'hui?",
        "Deutsch": "Guten Tag. Wie geht es Ihnen heute?",
        "Ğ ÑƒÑÑĞºĞ¸Ğ¹": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞšĞ°Ğº Ñ‚Ğ²Ğ¾Ğ¸ Ğ´ĞµĞ»Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ?",
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ù…Ø±Ø­Ø¨Ø§! ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
        "à¤¹à¤¿à¤¨à¥à¤¦à¥€": "à¤¨à¤®à¤¸à¥à¤¤à¥‡à¥¤ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?",
        "PortuguÃªs": "OlÃ¡! Como vocÃª estÃ¡ hoje?",
    },
    
    "ğŸŒ í¬ì†Œ ì–¸ì–´": {
        "Swahili": "Habari! Unaendeleaje leo?",
        "isiZulu": "Sawubona! Unjani namuhla?",
        "áŠ áˆ›áˆ­áŠ›": "áˆ°áˆ‹áˆ! áŠ¥áŠ•á‹´á‰µ áŠáˆ…?",
        "áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜": "áƒ’áƒáƒ›áƒáƒ áƒ¯áƒáƒ‘áƒ! áƒ áƒáƒ’áƒáƒ  áƒ®áƒáƒ ?",
        "Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶": "Ô²Õ¡Ö€Ö‡! Ô»Õ¶Õ¹ÕºÕ¥Õ½ Õ¥Õ½ Õ¡ÕµÕ½Ö…Ö€?",
        "×¢×‘×¨×™×ª": "×©×œ×•×! ××” ×©×œ×•××š ×”×™×•×?",
        "ÙØ§Ø±Ø³ÛŒ": "Ø³Ù„Ø§Ù…! Ø­Ø§Ù„ Ø´Ù…Ø§ Ú†Ø·ÙˆØ± Ø§Ø³ØªØŸ",
        "à¦¬à¦¾à¦‚à¦²à¦¾": "à¦¹à§à¦¯à¦¾à¦²à§‹! à¦†à¦ªà¦¨à¦¿ à¦•à§‡à¦®à¦¨ à¦†à¦›à§‡à¦¨?",
        "à®¤à®®à®¿à®´à¯": "à®µà®£à®•à¯à®•à®®à¯! à®¨à¯€à®™à¯à®•à®³à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?",
        "à¹„à¸—à¸¢": "à¸ªà¸§à¸±à¸ªà¸”à¸µ! à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡?",
        "Tiáº¿ng Viá»‡t": "Xin chÃ o! Báº¡n khá»e khÃ´ng?",
        "Bahasa Indonesia": "Halo! Apa kabar?",
        "TÃ¼rkÃ§e": "Merhaba! NasÄ±lsÄ±nÄ±z?",
        "Polski": "CzeÅ›Ä‡! Jak siÄ™ masz?",
        "Nederlands": "Hallo! Hoe gaat het?",
        "Î•Î»Î»Î·Î½Î¹ÎºÎ¬": "Î“ÎµÎ¹Î± ÏƒÎ¿Ï…! Î ÏÏ‚ ÎµÎ¯ÏƒÎ±Î¹;",
        "ÄŒeÅ¡tina": "Ahoj! Jak se mÃ¡Å¡?",
        "Magyar": "Szia! Hogy vagy?",
        "RomÃ¢nÄƒ": "BunÄƒ! Ce mai faci?",
        "Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°": "ĞŸÑ€Ğ¸Ğ²Ñ–Ñ‚! Ğ¯Ğº ÑĞ¿Ñ€Ğ°Ğ²Ğ¸?",
    },
    
    "ğŸ¯ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤": {
        "Emoji": "ğŸ˜ŠğŸ‰ğŸŒŸğŸ’–ğŸš€",
        "Mixed": "Hello ì•ˆë…• ä½ å¥½ ã“ã‚“ã«ã¡ã¯",
        "Numbers": "1234567890 ì¼ì´ì‚¼ì‚¬",
        "Symbols": "@#$%^&*()_+-=[]{}",
        "Korean Jamo": "ã„±ã„´ã„·ã„¹ã…ã…‚ã……ã…‡ã…ˆã…Šã…‹ã…Œã…ã…",
        "URL": "https://example.com/test",
        "Email": "test@example.com",
    }
}

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
all_results = []

for category, samples in test_samples.items():
    print(f"\n{category}")
    print("-" * 50)
    
    category_results = []
    
    for lang, text in samples.items():
        result = test_language(text, lang)
        category_results.append(result)
        all_results.append(result)
        
        print(f"{lang:20} TF: {result['tf_accuracy']:5.1%}  Char: {result['char_accuracy']:5.1%}  Loss: {result['loss']:.3f}")
        if result['char_accuracy'] < 0.5:
            print(f"  â†’ Reconstructed: {result['reconstructed'][:30]}...")
    
    # ì¹´í…Œê³ ë¦¬ í‰ê· 
    avg_tf = sum(r['tf_accuracy'] for r in category_results) / len(category_results)
    avg_char = sum(r['char_accuracy'] for r in category_results) / len(category_results)
    print(f"\nì¹´í…Œê³ ë¦¬ í‰ê· : TF {avg_tf:.1%}, Char {avg_char:.1%}")

# ì „ì²´ í†µê³„
print("\n" + "="*70)
print("ì „ì²´ í†µê³„")
print("="*70)

# ì–¸ì–´ë³„ ê·¸ë£¹ í†µê³„
major_langs = ["English", "í•œêµ­ì–´", "ä¸­æ–‡ç®€ä½“", "æ—¥æœ¬èª", "EspaÃ±ol", "FranÃ§ais", "Deutsch", "Ğ ÑƒÑÑĞºĞ¸Ğ¹", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"]
major_results = [r for r in all_results if r['lang'] in major_langs]
minor_results = [r for r in all_results if r['lang'] not in major_langs and r['lang'] not in ["Emoji", "Mixed", "Numbers", "Symbols", "Korean Jamo", "URL", "Email"]]

if major_results:
    avg_major_tf = sum(r['tf_accuracy'] for r in major_results) / len(major_results)
    avg_major_char = sum(r['char_accuracy'] for r in major_results) / len(major_results)
    print(f"ì£¼ìš” ì–¸ì–´ í‰ê· : TF {avg_major_tf:.1%}, Char {avg_major_char:.1%}")

if minor_results:
    avg_minor_tf = sum(r['tf_accuracy'] for r in minor_results) / len(minor_results)
    avg_minor_char = sum(r['char_accuracy'] for r in minor_results) / len(minor_results)
    print(f"í¬ì†Œ ì–¸ì–´ í‰ê· : TF {avg_minor_tf:.1%}, Char {avg_minor_char:.1%}")

# ê°œë³„ ì–¸ì–´ ìƒì„¸
print("\nê°œë³„ ì–¸ì–´ ì„±ëŠ¥:")
lang_groups = {
    "ì™„ë²½ (95%+)": [],
    "ìš°ìˆ˜ (80-95%)": [],
    "ì–‘í˜¸ (60-80%)": [],
    "ê°œì„  í•„ìš” (<60%)": []
}

for r in all_results:
    if r['char_accuracy'] >= 0.95:
        lang_groups["ì™„ë²½ (95%+)"].append(r['lang'])
    elif r['char_accuracy'] >= 0.80:
        lang_groups["ìš°ìˆ˜ (80-95%)"].append(r['lang'])
    elif r['char_accuracy'] >= 0.60:
        lang_groups["ì–‘í˜¸ (60-80%)"].append(r['lang'])
    else:
        lang_groups["ê°œì„  í•„ìš” (<60%)"].append(r['lang'])

for group, langs in lang_groups.items():
    if langs:
        print(f"\n{group}: {', '.join(langs[:10])}")
        if len(langs) > 10:
            print(f"  ... ì™¸ {len(langs)-10}ê°œ")

# JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
results_json = {
    'epoch': checkpoint['epoch'],
    'loss': float(checkpoint['loss']),
    'languages_tested': len(all_results),
    'average_tf_accuracy': sum(r['tf_accuracy'] for r in all_results) / len(all_results),
    'average_char_accuracy': sum(r['char_accuracy'] for r in all_results) / len(all_results),
    'language_results': all_results
}

with open('multilingual_test_results.json', 'w', encoding='utf-8') as f:
    json.dump(results_json, f, ensure_ascii=False, indent=2)

print(f"\nê²°ê³¼ê°€ multilingual_test_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("="*70)